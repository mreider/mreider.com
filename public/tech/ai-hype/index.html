<!DOCTYPE html>
<html lang="en">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="UTF-8">
    <title>Why AI won&#39;t take control - MR</title>
    <meta name="description" content="Co-Pilot vs. Autopilot in high-stakes environments">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    
    <meta property="og:image" content="http://localhost:1313/images/matt/copilot.jpg" />
    
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <meta name="twitter:card" content="summary_large_image" />
    
    <meta name="twitter:image" content="http://localhost:1313/images/matt/copilot.jpg" />
    

    <meta property="og:site_name" content="MR">

    
    <link rel="icon" type="image/svg+xml" href="/images/favicon.svg">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="shortcut icon" href="/images/favicon.ico">

    <link rel="me" href="https://github.com/mreider">

    <link rel="stylesheet" href="/css/custom.css">
</head>
<body>

<div class="container-lg mt-5">
    
    <header class="masthead">
        <h3 class="masthead-title">
            <a href="/" title="Home">MR</a>
        </h3>
    </header>

    
    <nav class="top-nav">
        
        
        <span class="nav-item"><a href="/" >Home</a></span>
        <span class="nav-item"><a href="/life/" >Life</a></span>
        <span class="nav-item"><a href="/tech/" class="active">Tech</a></span>
        <span class="nav-item"><a href="/writeups/" >Writeups</a></span>
        <span class="nav-item"><a href="/family/" >Family</a></span>
        <span class="nav-item"><a href="/about/" >About</a></span>
    </nav>

    
    <main>
        

<article>
    <header class="post-header">
        
        <img src="/images/matt/copilot.jpg" class="img-fluid mb-3 rounded" style="max-width: 300px;" alt="Why AI won&#39;t take control">
        

        <h1>Why AI won&#39;t take control</h1>

        <div class="post-meta">
            
            <time>May 10, 2025</time>
            
            
            ¬∑ by <a href="/authors/matt-reider/">Matt Reider</a>
            
        </div>
    </header>

    <div class="post-content">
        <p>AI went from a niche tech topic to a mainstream obsession in just the last 24 months. Unlike other hyped technologies that promised future revolutions, AI delivers immediate value anyone can experience. People can jump onto ChatGPT and use it to write a poem, draft an email, or provide information that previously required a Google search. This immediate utility has transformed AI from industry jargon into a household term.</p>
<p>Let&rsquo;s clarify what we&rsquo;re discussing: Autonomous AI operates independently without human oversight. Agentic AI refers to multiple AI systems working together autonomously. AGI (Artificial General Intelligence) means human-level intelligence across all domains. While Autonomous and Agentic AI exist today but have limited application in high-stakes situations, AGI doesn&rsquo;t exist at all - though many discuss it as inevitable without defining what it actually means.</p>
<p>My focus is on Autonomous and Agentic AI. These technologies exist today in controlled environments, but they won&rsquo;t be used for moderate to high-risk situations. The barriers aren&rsquo;t technical. They&rsquo;re practical and unavoidable.</p>
<p>Back to those podcasts - a prime example of how make-believe gets legitimized. The Cloudcast recently focused on <a href="https://www.thecloudcast.net/2025/04/the-intersection-of-ai-and-apis.html">MCP servers and Agentic infrastructure</a> as if companies are already using autonomous AIs that talk to each other. Maybe some are, but for anything important, that idea is dead on arrival. Hard Fork discusses AGI as inevitable, with cohost Kevin Roose even <a href="https://bsky.app/profile/kevinroose.com/post/3lne4qlnylc2d">writing a book about it</a>. They interview AI leaders like <a href="https://www.nytimes.com/2023/11/20/podcasts/mayhem-at-openai-our-interview-with-sam-altman.html">Sam Altman</a> and <a href="https://www.nytimes.com/2025/02/28/podcasts/hardfork-anthropic-dario-amodei.html">Dario Amodei</a> who talk about human-level machine intelligence as a done deal. Microsoft&rsquo;s CEO Satya Nadella adds fuel by <a href="https://www.youtube.com/watch?v=quXuKnGnOMs">claiming &ldquo;Agentic AI&rdquo; will replace all SaaS software</a>.</p>
<p>I don&rsquo;t buy it.</p>
<p>Last year, I listened to <a href="https://www.youtube.com/watch?v=csY-PH0CEHk">an episode of #shifthappens with Martin Eekels</a> who described a more realistic AI future. Eekels said that AI works best as &ldquo;a copilot to human capabilities rather than a replacement.&rdquo; The key point is that AI&rsquo;s limitations aren&rsquo;t technical. They&rsquo;re about risk.</p>
<p>Autonomous or &ldquo;agentic&rdquo; AI will happen. It will go beyond ChatGPT&rsquo;s current ability to suggest vacation spots. It will book refundable airline tickets, reserve cars, and secure hotel rooms by communicating with other AI systems across different organizations in an interoperable network. But in these scenarios of AI-to-AI interactions, the word &ldquo;refundable&rdquo; becomes crucial. When mistakes have minimal consequences, letting AI take control is no problem.</p>
<p>But for anything with serious consequences, both agentic and autopilot AI models have limitations. (While there are technical differences between networked AI systems and fully autonomous ones, they face the same risk limitations.) When real stakes are involved, the future promised by podcasters, tech bloggers, CEOs, and VCs doesn&rsquo;t hold up.</p>
<h2 id="the-automation-bargain">The Automation Bargain</h2>
<p>I experienced these tradeoffs firsthand last week, which inspired this post. I built a tool comparing message brokers (SQS, RabbitMQ, Kafka) with various instrumentations across multiple cloud platforms. Using <a href="https://www.anthropic.com/claude/sonnet">Anthropic&rsquo;s Sonnet LLM</a>, I completed the entire project in about a week, despite not being an engineer. Without AI, it would have been a year-long side project.</p>
<p>My project was purely a prototype with zero real-world risk. It involved no customer data, processed no financial transactions, and required no regulatory compliance. In this controlled environment where failure had no consequences, I confidently clicked the &ldquo;Automated Approval&rdquo; button in my VSCode plugin, <a href="https://cline.bot/">Cline</a>, letting Anthropic handle everything from building code to pushing it to my Kubernetes cluster.</p>
<p>Comparing my throw-away prototype to an autonomous AI taking control of a production system reveals unacceptable legal, financial, reputational, and human risks. This is why the co-pilot versus autopilot distinction matters. While I happily let AI assist me, I cannot imagine giving it complete autonomy to commit to git, approve its own pull requests, and deploy to production.</p>
<h2 id="the-5-problem-a-real-world-example">The 5% problem. A Real-World Example</h2>
<p>At a previous startup, I led a team of engineers as PM for an automated identity verification system. Our challenge was delivering instant results without mistakes that could directly harm people&rsquo;s lives.</p>
<p>Our engineering team built an AI neural network to evaluate probability matches between public records and user profiles. The system performed as well as humans for 95% of cases while providing instantaneous results. This met our business requirements for rapid turnaround and made our customers happy.</p>
<p>For the remaining 5% of cases, the system was hit-or-miss. In AI terms, that 5% represented our confusion matrix, instances where the AI couldn&rsquo;t confidently determine the answer. We ensured human review for each of these cases. Despite two years of intense effort under an ambitious CEO, we never reduced that confusion rate below 5%. According to friends still working there years later, they still can&rsquo;t.</p>
<p>The 5% problem wasn&rsquo;t a technical limitation. It was a risk management reality. The AI could make decisions autonomously if we let it. But the consequences of being wrong on that 5% were too severe to allow automation without human oversight. This is why &ldquo;agentic AI&rdquo; remains a pipe dream for any domain involving significant risk. The 5% barrier isn&rsquo;t about capability but about accountability.</p>
<p>Hypothetically, only two scenarios could eliminate our 5% verification problem. The first is a surveillance state with perfect data collection, where our public record verification would be flawless because records would already link directly to individuals. This would also make AI verification unnecessary.</p>
<p>The second scenario is a monopolistic or fully state-controlled economy where our AI&rsquo;s verification risks become irrelevant because customers would have nowhere else to go when the software made critical errors.</p>
<p>Both these scenarios typically exist under authoritarian regimes and represent the ambitions of certain powerful figures in technology and politics.</p>
<h2 id="back-to-reality">Back to reality</h2>
<p>AI will remain a co-pilot rather than an autopilot, at least in any moderately risky situation, for the foreseeable future. The real-world consequences of errors are too high for societies that have laws that protect their citizens.</p>
<p>The &ldquo;agentic AI revolution&rdquo; that tech leaders promise will likely follow the same path as &ldquo;mortgages on the blockchain.&rdquo; Despite all the promises of a future where we can sit back while AI handles everything, when it comes to meaningful work, humans will stay in the driver&rsquo;s seat while AI rides shotgun.</p>

    </div>

    <nav class="post-nav">
        <div>
            
            <a href="http://localhost:1313/tech/linked-in-refuge/">‚Üê LinkedIn, Geopolitics, and Silence</a>
            
        </div>
        <div>
            
        </div>
    </nav>
</article>


    </main>

    
    <footer class="site-footer text-center">
        
<p class="webring">
    <a href="https://xn--sr8hvo.ws/previous" rel="nofollow">‚Üê</a>
    An <a href="https://xn--sr8hvo.ws/">IndieWeb Webring</a> üï∏üíç
    <a href="https://xn--sr8hvo.ws/next" rel="nofollow">‚Üí</a>
</p>

        <p>Made with <a href="https://gohugo.io">Hugo</a></p>
    </footer>
</div>

</body>
</html>
